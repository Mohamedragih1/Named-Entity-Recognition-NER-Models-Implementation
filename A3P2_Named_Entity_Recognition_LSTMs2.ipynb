{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamedragih1/Named-Entity-Recognition-NER-Models-Implementation/blob/main/A3P2_Named_Entity_Recognition_LSTMs2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c9d0b3-e06d-4ab3-fd12-d4c00c5fa7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 28 20:03:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dRVuiP_JVdT"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5711ef5-04bf-434f-ec34-06bbf9d88d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "7e40edef-f302-4e02-ca8a-c75419476582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "2d84aa71b6024c9b9e3e48b3a0d61146"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00aac332-f2e1-434b-9b7b-23ad5eba90bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "        for i, sent in enumerate(sentences):\n",
        "          for j, element in enumerate(sent):\n",
        "            w, t = element\n",
        "            word_idx = self.words_vocab.lookup_indices([w])\n",
        "            if word_idx:\n",
        "               word_idxs[i, j] = word_idx[0]  # Assuming lookup_indices returns a list of indices\n",
        "            else:\n",
        "               word_idxs[i, j] = self.words_vocab.unknown_idx\n",
        "            tag_idx = self.tags_vocab.lookup_indices([t])\n",
        "            tag_idxs[i,j] = tag_idx[0]\n",
        "            valid_mask[i, j] = w in [word for word, _ in sent]\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ],
      "metadata": {
        "id": "Dq41V_uoinBS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8665a74d-6989-4ef7-92af-85513575504b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['China', 'on', 'Thursday', 'accused', 'Taipei', 'of', 'spoiling', 'the', 'atmosphere', 'for', 'a', 'resumption', 'of', 'talks', 'across', 'the', 'Taiwan', 'Strait', 'with', 'a', 'visit', 'to', 'Ukraine', 'by', 'Taiwanese', 'Vice', 'President', 'Lien', 'Chan', 'this', 'week', 'that', 'infuriated', 'Beijing', '.'], ['Speaking', 'only', 'hours', 'after', 'Chinese', 'state', 'media', 'said', 'the', 'time', 'was', 'right', 'to', 'engage', 'in', 'political', 'talks', 'with', 'Taiwan', ',', 'Foreign', 'Ministry', 'spokesman', 'Shen', 'Guofang', 'told', 'Reuters', ':', '\"', 'The', 'necessary', 'atmosphere', 'for', 'the', 'opening', 'of', 'the', 'talks', 'has', 'been', 'disrupted', 'by', 'the', 'Taiwan', 'authorities', '.', '\"'], ['State', 'media', 'quoted', 'China', \"'s\", 'top', 'negotiator', 'with', 'Taipei', ',', 'Tang', 'Shubei', ',', 'as', 'telling', 'a', 'visiting', 'group', 'from', 'Taiwan', 'on', 'Wednesday', 'that', 'it', 'was', 'time', 'for', 'the', 'rivals', 'to', 'hold', 'political', 'talks', '.']], 'word_idxs': tensor([[345,  19,  20, 156, 352, 163, 353,  41, 349,  71,  80, 354, 163, 350,\n",
            "         210,  41, 347, 355,  23,  80, 356,   6, 357,  94, 358, 359, 360, 361,\n",
            "         362, 363, 364,  90, 365, 366,  10,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0],\n",
            "        [367, 146, 368, 120, 369, 370, 371,  18,  41, 372,  57, 373,   6, 374,\n",
            "         237, 375, 350,  23, 347,  72, 376, 377,  74, 378, 379,  79, 380, 381,\n",
            "          59,  15, 382, 349,  71,  41, 383, 163,  41, 350, 230, 186, 384,  94,\n",
            "          41, 347, 385,  10,  59],\n",
            "        [386, 371, 387, 345,  39, 388, 389,  23, 352,  72, 390, 391,  72, 144,\n",
            "         289,  80, 392, 393,  51, 347,  19,  47,  90,  21,  57, 372,  71,  41,\n",
            "         394,   6, 395, 375, 350,  10,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0]]), 'tags': [['LOC', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'O', 'MISC', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'LOC', 'O'], ['O', 'O', 'O', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'ORG', 'ORG', 'O', 'PER', 'PER', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O'], ['O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'LOC', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']], 'tag_idxs': tensor([[4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 4, 1,\n",
            "         2, 1, 1, 3, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 0, 0, 1, 3,\n",
            "         3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1],\n",
            "        [1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 10:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
        "        self.LSTM = nn.LSTM(d_emb ,d_hidden, bidirectional= bidirectional)\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.linear = nn.Linear(d_hidden* num_directions, len(tags_vocab))\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        embeddings = self.embedding(word_idxs) #Output dimensions: (N,L,d_emb)\n",
        "        hidden,_ = self.LSTM(embeddings) #hidden dimension: (L,N,d_hidden*num_directions)\n",
        "        #valid_mask = valid_mask.int()\n",
        "        #hidden_masked = hidden * valid_mask.unsqueeze(-1)\n",
        "        logits = self.linear(hidden) #ouput of the linear layer (L,N,5)\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c037ba-2d3b-4a74-ea01-18435ee67d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (LSTM): LSTM(64, 128, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 15])\n",
            "Input valid_mask shape: torch.Size([4, 15])\n",
            "Output logits shape: torch.Size([4, 15, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        logits = model.forward(word_idxs, valid_mask)\n",
        "        #print(logits.size())\n",
        "        #masked_logits = logits[valid_mask]\n",
        "        #masked_tags =  logits[valid_mask]\n",
        "        #loss  = F.cross_entropy(masked_logits, masked_tags)\n",
        "        loss = F.cross_entropy(logits.transpose(1, 2), tag_idxs, reduction='none')\n",
        "        loss = loss * valid_mask.float()\n",
        "        loss = loss.sum()/valid_mask.sum()\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "           print(\"\\t[%06d/%06d] Loss: %f\"\n",
        "            % (i, len(loader), np.mean([loss\n",
        "             for loss in losses[-report_interval:]])))\n",
        "\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            logits = model.forward(word_idxs, valid_mask)\n",
        "            #masked_logits = logits[valid_mask]\n",
        "            #masked_tags =  logits[valid_mask]\n",
        "            #loss  = F.cross_entropy(masked_logits, masked_tags)\n",
        "            loss = F.cross_entropy(logits.transpose(1, 2), tag_idxs, reduction='none')\n",
        "            loss = loss * valid_mask.float()\n",
        "            loss = loss.sum()/valid_mask.sum()\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70c6b53-34ac-4816-c0b7-e85f7200cfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (LSTM): LSTM(64, 128, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7874426698243177\n",
            "Training metrics:\n",
            "\t accuracy :  0.7818773873757069\n",
            "\t f1 :  [0.05009393 0.88489856 0.01643749 0.09672023 0.16984881]\n",
            "\t average f1 :  0.24359980346986182\n",
            "\t confusion matrix :  [[   360   8216    135    676    473]\n",
            " [  3519 154444   1386   6309   1334]\n",
            " [   157   3750     52    305    276]\n",
            " [   256   9302    121    954    327]\n",
            " [   221   6362     93    523    983]]\n",
            "Validating..\n",
            "Validation loss:  0.4090628113065447\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8649716823534205\n",
            "\t f1 :  [0.11025922 0.94705459 0.         0.23376623 0.50319611]\n",
            "\t average f1 :  0.358855231737201\n",
            "\t confusion matrix :  [[  151  1207     0   405   487]\n",
            " [   80 40819     0   202    63]\n",
            " [   70   486     0   204   247]\n",
            " [   74  1956     1   504   155]\n",
            " [  114   570     0   307   984]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.30872877273294663\n",
            "Training metrics:\n",
            "\t accuracy :  0.8937693151231183\n",
            "\t f1 :  [0.32023484 0.97130742 0.17328094 0.50702811 0.60086145]\n",
            "\t average f1 :  0.514542552825892\n",
            "\t confusion matrix :  [[  2400   2629     54   2999   1770]\n",
            " [   370 165385      9   1166    199]\n",
            " [   732    974    441   1426    957]\n",
            " [   910   3348     23   6060    596]\n",
            " [   725   1076     33   1316   5022]]\n",
            "Validating..\n",
            "Validation loss:  0.30353908453668865\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9024161675426802\n",
            "\t f1 :  [0.39796192 0.96439404 0.49327354 0.4501463  0.70750135]\n",
            "\t average f1 :  0.6026554294278031\n",
            "\t confusion matrix :  [[  742   722    88   379   319]\n",
            " [  110 40858    10   171    15]\n",
            " [  182   272   385   109    59]\n",
            " [  216  1417    30  1000    27]\n",
            " [  229   300    41    94  1311]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.19912814155772882\n",
            "Training metrics:\n",
            "\t accuracy :  0.9372500661497831\n",
            "\t f1 :  [0.57309573 0.98760297 0.62598425 0.72548934 0.76554964]\n",
            "\t average f1 :  0.7355443856820079\n",
            "\t confusion matrix :  [[  5451   1052    402   1810   1164]\n",
            " [   452 165503     75    727     63]\n",
            " [   781    436   2385    571    331]\n",
            " [  1394   1003    123   8284    137]\n",
            " [  1066    347    131    504   6111]]\n",
            "Validating..\n",
            "Validation loss:  0.266404213649886\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9201401621643646\n",
            "\t f1 :  [0.5763873  0.96780144 0.63795256 0.5507459  0.76082649]\n",
            "\t average f1 :  0.6987427398582928\n",
            "\t confusion matrix :  [[ 1262   665    40   123   160]\n",
            " [  127 40923    26    79     9]\n",
            " [  187   251   511    30    28]\n",
            " [  243  1302     2  1126    17]\n",
            " [  310   264    16    41  1344]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.13498009962064247\n",
            "Training metrics:\n",
            "\t accuracy :  0.9606002347007565\n",
            "\t f1 :  [0.72786647 0.99238929 0.76976027 0.85683713 0.82249781]\n",
            "\t average f1 :  0.8338701926249421\n",
            "\t confusion matrix :  [[  7021    721    343    853    900]\n",
            " [   328 165926     94    389     39]\n",
            " [   422    330   3243    290    239]\n",
            " [   718    463    105   9609     65]\n",
            " [   965    181    117    328   6566]]\n",
            "Validating..\n",
            "Validation loss:  0.23957157347883498\n",
            "Validation metrics:\n",
            "\t accuracy :  0.929654076518763\n",
            "\t f1 :  [0.63957863 0.96961663 0.72364672 0.62064156 0.78439989]\n",
            "\t average f1 :  0.7475766867620741\n",
            "\t confusion matrix :  [[ 1275   626    48   123   178]\n",
            " [   83 40960    32    77    12]\n",
            " [   62   242   635    37    31]\n",
            " [   83  1243    12  1335    17]\n",
            " [  234   252    21    40  1428]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.10255437371907411\n",
            "Training metrics:\n",
            "\t accuracy :  0.9700252240755326\n",
            "\t f1 :  [0.78420427 0.99460057 0.81048812 0.91378693 0.84507566]\n",
            "\t average f1 :  0.8696311097247371\n",
            "\t confusion matrix :  [[  7586    620    296    517    852]\n",
            " [   251 166521     82    195     37]\n",
            " [   335    288   3462    239    198]\n",
            " [   408    214     70  10207     60]\n",
            " [   896    121    111    223   6813]]\n",
            "Validating..\n",
            "Validation loss:  0.2531401813030243\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9327303100680439\n",
            "\t f1 :  [0.65754119 0.97036266 0.73137698 0.6422707  0.79805616]\n",
            "\t average f1 :  0.7599215356987923\n",
            "\t confusion matrix :  [[ 1297   616    45   103   189]\n",
            " [   73 40992    41    47    11]\n",
            " [   62   240   648    23    34]\n",
            " [   63  1229    12  1369    17]\n",
            " [  200   247    19    31  1478]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.08348771571009247\n",
            "Training metrics:\n",
            "\t accuracy :  0.974893018523506\n",
            "\t f1 :  [0.81750772 0.99551669 0.82766597 0.94194307 0.86160467]\n",
            "\t average f1 :  0.888847626109713\n",
            "\t confusion matrix :  [[  7938    559    282    322    803]\n",
            " [   227 166537     90     96     29]\n",
            " [   298    267   3578    181    203]\n",
            " [   278    129     54  10408     61]\n",
            " [   775    103    115    162   7007]]\n",
            "Validating..\n",
            "Validation loss:  0.2415543794631958\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9337896752638227\n",
            "\t f1 :  [0.66542355 0.97034114 0.74307863 0.64614683 0.81085312]\n",
            "\t average f1 :  0.7671686531080799\n",
            "\t confusion matrix :  [[ 1249   590    55   120   236]\n",
            " [   60 40945    46    96    17]\n",
            " [   36   236   671    30    34]\n",
            " [   31  1214    11  1417    17]\n",
            " [  128   244    16    33  1554]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.0733511558285466\n",
            "Training metrics:\n",
            "\t accuracy :  0.9777161276158353\n",
            "\t f1 :  [0.83444946 0.99606349 0.84516799 0.95608169 0.87161424]\n",
            "\t average f1 :  0.9006753741932846\n",
            "\t confusion matrix :  [[  8037    535    275    250    785]\n",
            " [   196 166495     94     63     27]\n",
            " [   248    242   3660    133    222]\n",
            " [   202     75     39  10580     70]\n",
            " [   698     84     88    140   7176]]\n",
            "Validating..\n",
            "Validation loss:  0.2627221473625728\n",
            "Validation metrics:\n",
            "\t accuracy :  0.935399095465102\n",
            "\t f1 :  [0.68037102 0.9709629  0.7450537  0.66399055 0.79680617]\n",
            "\t average f1 :  0.7714368683280798\n",
            "\t confusion matrix :  [[ 1357   617    45    73   158]\n",
            " [   65 41046    26    21     6]\n",
            " [   48   253   659    18    29]\n",
            " [   33  1223    11  1406    17]\n",
            " [  236   244    21    27  1447]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.06629747290302206\n",
            "Training metrics:\n",
            "\t accuracy :  0.9793457747955546\n",
            "\t f1 :  [0.84552507 0.99611575 0.85740465 0.96532317 0.87846378]\n",
            "\t average f1 :  0.9085664834124219\n",
            "\t confusion matrix :  [[  8120    536    249    176    777]\n",
            " [   196 166436     91     53     25]\n",
            " [   227    252   3743    108    194]\n",
            " [   147     71     31  10634     59]\n",
            " [   659     74     93    119   7228]]\n",
            "Validating..\n",
            "Validation loss:  0.27220521441527773\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9368047915902702\n",
            "\t f1 :  [0.68411314 0.97119059 0.74591058 0.66824085 0.8149513 ]\n",
            "\t average f1 :  0.7768812920827608\n",
            "\t confusion matrix :  [[ 1294   603    68    65   220]\n",
            " [   55 41043    39    17    10]\n",
            " [   29   241   684    20    33]\n",
            " [   24  1225    13  1415    13]\n",
            " [  131   245    23    28  1548]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.06170547643193492\n",
            "Training metrics:\n",
            "\t accuracy :  0.9808265106045412\n",
            "\t f1 :  [0.85531716 0.99639352 0.8686503  0.969791   0.88631119]\n",
            "\t average f1 :  0.9152926332229587\n",
            "\t confusion matrix :  [[  8232    514    277    150    729]\n",
            " [   173 166596     75     40     26]\n",
            " [   186    244   3839     83    191]\n",
            " [   140     60     29  10626     64]\n",
            " [   616     74     76     96   7297]]\n",
            "Validating..\n",
            "Validation loss:  0.27517224848270416\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9373141017805484\n",
            "\t f1 :  [0.6931729  0.9711378  0.74555556 0.67015458 0.8184423 ]\n",
            "\t average f1 :  0.7796926278964216\n",
            "\t confusion matrix :  [[ 1325   597    51    53   224]\n",
            " [   61 41033    41    17    12]\n",
            " [   34   245   671    19    38]\n",
            " [   26  1223    13  1409    19]\n",
            " [  127   243    17    17  1571]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.058067932862926414\n",
            "Training metrics:\n",
            "\t accuracy :  0.9816012083086172\n",
            "\t f1 :  [0.86070274 0.99647133 0.8742909  0.97321388 0.88930514]\n",
            "\t average f1 :  0.9187967977083202\n",
            "\t confusion matrix :  [[  8255    505    246    129    733]\n",
            " [   168 166753     79     47     28]\n",
            " [   191    237   3853     59    183]\n",
            " [   118     47     33  10700     63]\n",
            " [   582     70     80     93   7359]]\n",
            "Validating..\n",
            "Validation loss:  0.3010888546705246\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9377419223403822\n",
            "\t f1 :  [0.69628099 0.9712157  0.7509837  0.67320105 0.8168343 ]\n",
            "\t average f1 :  0.7817031502017548\n",
            "\t confusion matrix :  [[ 1348   615    45    42   200]\n",
            " [   54 41063    33     7     7]\n",
            " [   36   250   668    17    36]\n",
            " [   34  1220    11  1408    17]\n",
            " [  150   248    15    19  1543]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.05535359473692046\n",
            "Training metrics:\n",
            "\t accuracy :  0.9822579036601177\n",
            "\t f1 :  [0.86584285 0.99659819 0.88088518 0.97348433 0.89309698]\n",
            "\t average f1 :  0.9219815058620824\n",
            "\t confusion matrix :  [[  8303    493    243    127    697]\n",
            " [   162 166695     69     40     28]\n",
            " [   183    215   3901     53    188]\n",
            " [   120     56     30  10702     71]\n",
            " [   548     75     74     86   7381]]\n",
            "Validating..\n",
            "Validation loss:  0.2762519248894283\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9373955914109929\n",
            "\t f1 :  [0.68381565 0.97122779 0.75153546 0.67172316 0.81967213]\n",
            "\t average f1 :  0.7795948359384564\n",
            "\t confusion matrix :  [[ 1276   608    51    51   264]\n",
            " [   53 41047    31    22    11]\n",
            " [   32   247   673    18    37]\n",
            " [   28  1218    10  1417    17]\n",
            " [   93   242    19    21  1600]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.05406578850966913\n",
            "Training metrics:\n",
            "\t accuracy :  0.9825840259669032\n",
            "\t f1 :  [0.8678831  0.99668856 0.88280543 0.97486935 0.89357333]\n",
            "\t average f1 :  0.923163953082496\n",
            "\t confusion matrix :  [[  8300    482    253    125    706]\n",
            " [   146 166745     63     46     31]\n",
            " [   165    224   3902     52    189]\n",
            " [   111     40     24  10726     63]\n",
            " [   539     76     66     92   7397]]\n",
            "Validating..\n",
            "Validation loss:  0.29618587664195467\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9372122397424928\n",
            "\t f1 :  [0.68430802 0.97133796 0.74276352 0.67002882 0.81930185]\n",
            "\t average f1 :  0.7775480316378097\n",
            "\t confusion matrix :  [[ 1293   591    71    42   253]\n",
            " [   61 41040    38     9    16]\n",
            " [   36   244   680    12    35]\n",
            " [   41  1222    11  1395    21]\n",
            " [   98   241    24    16  1596]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.05235171000714655\n",
            "Training metrics:\n",
            "\t accuracy :  0.9829666111681604\n",
            "\t f1 :  [0.8713615  0.99666845 0.88371565 0.97659439 0.89625098]\n",
            "\t average f1 :  0.9249181955943151\n",
            "\t confusion matrix :  [[  8352    466    244    111    686]\n",
            " [   164 166782     57     44     31]\n",
            " [   161    234   3891     47    177]\n",
            " [   105     45     26  10765     59]\n",
            " [   529     74     78     79   7399]]\n",
            "Validating..\n",
            "Validation loss:  0.2864313061748232\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9371918673348817\n",
            "\t f1 :  [0.69008915 0.97124396 0.74741988 0.66905787 0.81597494]\n",
            "\t average f1 :  0.7787571597709289\n",
            "\t confusion matrix :  [[ 1316   599    68    47   220]\n",
            " [   55 41037    43    17    12]\n",
            " [   30   240   688    13    36]\n",
            " [   35  1219    12  1399    25]\n",
            " [  128   245    23    16  1563]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.05117702359954516\n",
            "Training metrics:\n",
            "\t accuracy :  0.9832184447361721\n",
            "\t f1 :  [0.87251829 0.99682741 0.88689937 0.97611015 0.89788817]\n",
            "\t average f1 :  0.9260486760840537\n",
            "\t confusion matrix :  [[  8350    473    245    116    688]\n",
            " [   142 166526     60     31     40]\n",
            " [   163    202   3913     44    187]\n",
            " [   119     43     22  10705     71]\n",
            " [   494     69     75     78   7483]]\n",
            "Validating..\n",
            "Validation loss:  0.2899377920797893\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9375178258566598\n",
            "\t f1 :  [0.6938246  0.9713204  0.74103139 0.67237687 0.81868276]\n",
            "\t average f1 :  0.7794472039172045\n",
            "\t confusion matrix :  [[ 1337   600    52    46   215]\n",
            " [   55 41048    34    17    10]\n",
            " [   48   245   661    17    36]\n",
            " [   34  1219     9  1413    15]\n",
            " [  130   244    21    20  1560]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.05066222435346356\n",
            "Training metrics:\n",
            "\t accuracy :  0.9832782257863423\n",
            "\t f1 :  [0.87399687 0.99677057 0.88668396 0.97662125 0.89824181]\n",
            "\t average f1 :  0.9264628905116299\n",
            "\t confusion matrix :  [[  8386    471    230    108    696]\n",
            " [   142 166672     75     39     33]\n",
            " [   166    211   3932     42    181]\n",
            " [   116     37     26  10715     71]\n",
            " [   489     72     74     74   7459]]\n",
            "Validating..\n",
            "Validation loss:  0.2735359157834734\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9374770810414375\n",
            "\t f1 :  [0.69866128 0.97143804 0.75418994 0.67254949 0.80811316]\n",
            "\t average f1 :  0.7809903814913879\n",
            "\t confusion matrix :  [[ 1383   581    44    45   197]\n",
            " [   66 41035    39    15     9]\n",
            " [   44   240   675    12    36]\n",
            " [   36  1219     9  1410    16]\n",
            " [  180   244    16    21  1514]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8101b569-5e9c-4bf8-bdb8-dd1d1dac1c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (LSTM): LSTM(64, 128)\n",
            "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.6503077436376501\n",
            "Training metrics:\n",
            "\t accuracy :  0.7905638722554891\n",
            "\t f1 :  [0.08066782 0.89269332 0.01018737 0.08323133 0.21190115]\n",
            "\t average f1 :  0.25573620188822493\n",
            "\t confusion matrix :  [[   488   7711     77    528   1072]\n",
            " [  1047 155205    766   3668   6216]\n",
            " [   162   3320     28    205    785]\n",
            " [   246   9087     69    680    856]\n",
            " [   280   5498     57    321   2028]]\n",
            "Validating..\n",
            "Validation loss:  0.39037356206348967\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8747300655991526\n",
            "\t f1 :  [0.30488137 0.95060096 0.         0.16829499 0.55316057]\n",
            "\t average f1 :  0.39538757818354714\n",
            "\t confusion matrix :  [[  559  1150     0   107   434]\n",
            " [  104 40969     0    24    67]\n",
            " [  201   421     0    48   337]\n",
            " [  286  1963     0   267   174]\n",
            " [  267   529     0    37  1142]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.29361383340976854\n",
            "Training metrics:\n",
            "\t accuracy :  0.9027889280603162\n",
            "\t f1 :  [0.41042576 0.97430874 0.33004926 0.54563948 0.63266998]\n",
            "\t average f1 :  0.5786186458477276\n",
            "\t confusion matrix :  [[  3480   2241    112   2494   1525]\n",
            " [   470 165120     25   1275    157]\n",
            " [   870    773    938    804   1139]\n",
            " [  1345   2857     27   6169    548]\n",
            " [   941    910     58    924   5341]]\n",
            "Validating..\n",
            "Validation loss:  0.280547633767128\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9050849529397385\n",
            "\t f1 :  [0.50809717 0.96751549 0.52243804 0.54556594 0.71740347]\n",
            "\t average f1 :  0.6522040216315046\n",
            "\t confusion matrix :  [[ 1004   444    36   560   206]\n",
            " [   64 39970    12  1099    19]\n",
            " [  183   175   390   190    69]\n",
            " [  186   690    10  1781    23]\n",
            " [  265   181    38   209  1282]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.1879166348112954\n",
            "Training metrics:\n",
            "\t accuracy :  0.9429434067549187\n",
            "\t f1 :  [0.62485543 0.98869307 0.6756474  0.74960803 0.77402696]\n",
            "\t average f1 :  0.7625661768147366\n",
            "\t confusion matrix :  [[  5943    929    274   1810    943]\n",
            " [   408 165745     72    924     41]\n",
            " [   657    353   2583    452    460]\n",
            " [  1161    780     44   8845    112]\n",
            " [   954    284    168    626   6145]]\n",
            "Validating..\n",
            "Validation loss:  0.23238036462238856\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9252536364747586\n",
            "\t f1 :  [0.61885454 0.96977836 0.67633411 0.59377664 0.77435198]\n",
            "\t average f1 :  0.7266191242032627\n",
            "\t confusion matrix :  [[ 1221   580    63   188   198]\n",
            " [   60 40801    31   261    11]\n",
            " [   88   225   583    68    43]\n",
            " [  113  1150    15  1393    19]\n",
            " [  214   225    25    92  1419]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.12920304829323734\n",
            "Training metrics:\n",
            "\t accuracy :  0.9631494461449495\n",
            "\t f1 :  [0.749974   0.99284028 0.77179425 0.8736302  0.82632564]\n",
            "\t average f1 :  0.8429128736856862\n",
            "\t confusion matrix :  [[  7211    674    304    870    830]\n",
            " [   274 166127     81    522     30]\n",
            " [   483    307   3196    243    300]\n",
            " [   479    338     33  10045     61]\n",
            " [   894    170    139    360   6623]]\n",
            "Validating..\n",
            "Validation loss:  0.21520899023328507\n",
            "Validation metrics:\n",
            "\t accuracy :  0.934156378600823\n",
            "\t f1 :  [0.64945792 0.97437681 0.71302428 0.68925926 0.79269603]\n",
            "\t average f1 :  0.7637628613200527\n",
            "\t confusion matrix :  [[ 1258   483    81   217   211]\n",
            " [   53 40613    31   459     8]\n",
            " [   56   196   646    75    34]\n",
            " [   74   718    17  1861    20]\n",
            " [  183   188    30    98  1476]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.10102801769971848\n",
            "Training metrics:\n",
            "\t accuracy :  0.9710024068951053\n",
            "\t f1 :  [0.79417901 0.99481893 0.80634548 0.92171751 0.84418949]\n",
            "\t average f1 :  0.8722500832677695\n",
            "\t confusion matrix :  [[  7613    584    305    515    847]\n",
            " [   219 166185     81    253     26]\n",
            " [   378    290   3431    165    260]\n",
            " [   290    163     38  10379     68]\n",
            " [   808    115    131    271   6843]]\n",
            "Validating..\n",
            "Validation loss:  0.21876999522958482\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9236645886810904\n",
            "\t f1 :  [0.66598097 0.97131057 0.7370812  0.64296336 0.79925749]\n",
            "\t average f1 :  0.7633187198734055\n",
            "\t confusion matrix :  [[ 1295   267    51   417   220]\n",
            " [   67 39510    30  1544    13]\n",
            " [   62    94   649   166    36]\n",
            " [   42   242     8  2378    20]\n",
            " [  173    77    16   202  1507]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.08530481490823957\n",
            "Training metrics:\n",
            "\t accuracy :  0.9750450796949066\n",
            "\t f1 :  [0.82229134 0.99541139 0.8269029  0.94309597 0.86042229]\n",
            "\t average f1 :  0.8896247796761992\n",
            "\t confusion matrix :  [[  7931    552    277    357    761]\n",
            " [   207 166169    105    148     23]\n",
            " [   298    295   3547    128    251]\n",
            " [   220    108     27  10549     65]\n",
            " [   756     94    104    220   7009]]\n",
            "Validating..\n",
            "Validation loss:  0.21977863567216055\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9187752108544188\n",
            "\t f1 :  [0.65473285 0.96956926 0.73801561 0.62636004 0.80804481]\n",
            "\t average f1 :  0.7593445116336996\n",
            "\t confusion matrix :  [[ 1207   206    60   488   289]\n",
            " [   54 39110    36  1949    15]\n",
            " [   47    63   662   195    40]\n",
            " [   30    94    11  2533    22]\n",
            " [   99    38    18   233  1587]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.0753755282472681\n",
            "Training metrics:\n",
            "\t accuracy :  0.9773223417298816\n",
            "\t f1 :  [0.83507307 0.99572203 0.83951473 0.95539452 0.87107718]\n",
            "\t average f1 :  0.8993563058810519\n",
            "\t confusion matrix :  [[  8000    566    283    273    773]\n",
            " [   189 166653     96     92     26]\n",
            " [   265    282   3633     96    257]\n",
            " [   165     90     25  10613     76]\n",
            " [   646     91     85    174   7189]]\n",
            "Validating..\n",
            "Validation loss:  0.21101709561688559\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9180010593651958\n",
            "\t f1 :  [0.69058737 0.96683961 0.74202574 0.58610792 0.79690949]\n",
            "\t average f1 :  0.7564940260105406\n",
            "\t confusion matrix :  [[ 1405   297    51   349   148]\n",
            " [   83 39507    32  1536     6]\n",
            " [   55    99   663   158    32]\n",
            " [   40   581     8  2042    19]\n",
            " [  236    76    26   193  1444]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.06856152705020374\n",
            "Training metrics:\n",
            "\t accuracy :  0.9791554491973278\n",
            "\t f1 :  [0.8472367  0.99586409 0.8544493  0.96404637 0.88000966]\n",
            "\t average f1 :  0.9083212235734475\n",
            "\t confusion matrix :  [[  8125    553    260    193    762]\n",
            " [   201 166623     99     76     28]\n",
            " [   225    281   3716     83    225]\n",
            " [   142     63     17  10645     70]\n",
            " [   594     83     76    150   7290]]\n",
            "Validating..\n",
            "Validation loss:  0.2140567217554365\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9189381901153079\n",
            "\t f1 :  [0.68631786 0.96595993 0.75070028 0.58249419 0.79445215]\n",
            "\t average f1 :  0.7559848816897634\n",
            "\t confusion matrix :  [[ 1397   369    47   288   149]\n",
            " [   73 39728    33  1325     5]\n",
            " [   48   129   670   127    33]\n",
            " [   51   739     9  1880    11]\n",
            " [  252   127    19   145  1432]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.06441940284437603\n",
            "Training metrics:\n",
            "\t accuracy :  0.9799251055851688\n",
            "\t f1 :  [0.8521721  0.99600557 0.86149647 0.96751489 0.88225297]\n",
            "\t average f1 :  0.911888402444907\n",
            "\t confusion matrix :  [[  8180    536    260    186    722]\n",
            " [   191 166565     96     55     30]\n",
            " [   203    275   3788     67    216]\n",
            " [   128     63     26  10722     70]\n",
            " [   612     90     75    125   7268]]\n",
            "Validating..\n",
            "Validation loss:  0.22095007768699101\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9225848510777004\n",
            "\t f1 :  [0.6611797  0.96678218 0.7459634  0.59293238 0.81190116]\n",
            "\t average f1 :  0.7557517647815806\n",
            "\t confusion matrix :  [[ 1205   434    80   225   306]\n",
            " [   50 40033    43  1025    13]\n",
            " [   26   147   693   102    39]\n",
            " [   34   877    11  1745    23]\n",
            " [   80   162    24    99  1610]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.06036997765854553\n",
            "Training metrics:\n",
            "\t accuracy :  0.9810119630125222\n",
            "\t f1 :  [0.85816687 0.99621598 0.86894198 0.96950416 0.88927944]\n",
            "\t average f1 :  0.9164216858719172\n",
            "\t confusion matrix :  [[  8141    522    244    163    724]\n",
            " [   177 166518    101     50     30]\n",
            " [   188    256   3819     64    194]\n",
            " [   141     44     26  10666     70]\n",
            " [   532     85     79    113   7337]]\n",
            "Validating..\n",
            "Validation loss:  0.2346286177635193\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9365603226989365\n",
            "\t f1 :  [0.66757941 0.9710735  0.75321768 0.67334109 0.81610942]\n",
            "\t average f1 :  0.7762642216843739\n",
            "\t confusion matrix :  [[ 1219   614    52    70   295]\n",
            " [   46 41023    31    52    12]\n",
            " [   29   252   673    13    40]\n",
            " [   26  1193    10  1446    15]\n",
            " [   82   244    14    24  1611]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.05802167069028925\n",
            "Training metrics:\n",
            "\t accuracy :  0.9813621858795373\n",
            "\t f1 :  [0.86191548 0.99628906 0.87379083 0.97048912 0.88910318]\n",
            "\t average f1 :  0.9183175336610878\n",
            "\t confusion matrix :  [[  8230    506    239    152    740]\n",
            " [   172 166722     92     51     30]\n",
            " [   176    249   3839     53    209]\n",
            " [   123     58     27  10655     73]\n",
            " [   529     84     64    111   7376]]\n",
            "Validating..\n",
            "Validation loss:  0.23602238297462463\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9368251639978813\n",
            "\t f1 :  [0.69039146 0.97121902 0.75495979 0.67367918 0.8005472 ]\n",
            "\t average f1 :  0.7781593279101313\n",
            "\t confusion matrix :  [[ 1358   616    64    42   170]\n",
            " [   51 41051    43    10     9]\n",
            " [   28   236   704    11    28]\n",
            " [   39  1221    11  1409    10]\n",
            " [  208   247    36    21  1463]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.05597586129550581\n",
            "Training metrics:\n",
            "\t accuracy :  0.9819970776419663\n",
            "\t f1 :  [0.86494689 0.99642179 0.87860618 0.97275167 0.89329636]\n",
            "\t average f1 :  0.9212045778097359\n",
            "\t confusion matrix :  [[  8265    521    242    140    714]\n",
            " [   154 166664     93     38     33]\n",
            " [   172    238   3883     52    180]\n",
            " [   127     45     18  10692     68]\n",
            " [   511     75     78    111   7409]]\n",
            "Validating..\n",
            "Validation loss:  0.23380035161972046\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9370085156663814\n",
            "\t f1 :  [0.69246333 0.97118913 0.75321768 0.67157795 0.80994387]\n",
            "\t average f1 :  0.7796783905266352\n",
            "\t confusion matrix :  [[ 1369   597    46    43   195]\n",
            " [   69 41024    34    30     7]\n",
            " [   45   239   673    15    35]\n",
            " [   41  1212    10  1413    14]\n",
            " [  180   246    17    17  1515]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.05414534442954593\n",
            "Training metrics:\n",
            "\t accuracy :  0.982178445309105\n",
            "\t f1 :  [0.86663532 0.99645488 0.87976473 0.9736662  0.89364532]\n",
            "\t average f1 :  0.9220332903776509\n",
            "\t confusion matrix :  [[  8295    497    242    140    688]\n",
            " [   157 166398     78     36     40]\n",
            " [   179    238   3889     52    174]\n",
            " [   120     49     21  10667     72]\n",
            " [   530     89     79     87   7390]]\n",
            "Validating..\n",
            "Validation loss:  0.2405232467821666\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9179806869575846\n",
            "\t f1 :  [0.67456258 0.96488494 0.76190476 0.57343511 0.81518987]\n",
            "\t average f1 :  0.7579954544913707\n",
            "\t confusion matrix :  [[ 1253   356    55   294   292]\n",
            " [   59 39623    45  1424    13]\n",
            " [   30   121   696   119    41]\n",
            " [   34   748    11  1878    19]\n",
            " [   89   118    13   145  1610]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0529256974933324\n",
            "Training metrics:\n",
            "\t accuracy :  0.9829801235361859\n",
            "\t f1 :  [0.87277659 0.99659919 0.88778168 0.9747072  0.89733245]\n",
            "\t average f1 :  0.9258394204511282\n",
            "\t confusion matrix :  [[  8366    500    219    125    696]\n",
            " [   138 166744     73     46     37]\n",
            " [   166    217   3920     44    180]\n",
            " [   110     45     26  10694     68]\n",
            " [   485     82     66     91   7451]]\n",
            "Validating..\n",
            "Validation loss:  0.2626818801675524\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9367640467750479\n",
            "\t f1 :  [0.6911315  0.97119595 0.75027747 0.66506256 0.80992608]\n",
            "\t average f1 :  0.7775187116987125\n",
            "\t confusion matrix :  [[ 1356   592    48    40   214]\n",
            " [   58 41034    42    20    10]\n",
            " [   40   245   676    11    35]\n",
            " [   56  1223     9  1382    20]\n",
            " [  164   244    20    13  1534]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.05203519416628061\n",
            "Training metrics:\n",
            "\t accuracy :  0.983029788932688\n",
            "\t f1 :  [0.87086804 0.996634   0.8864047  0.97638081 0.89864134]\n",
            "\t average f1 :  0.9257857783156844\n",
            "\t confusion matrix :  [[  8312    491    246    127    699]\n",
            " [   151 166550     75     33     40]\n",
            " [   159    215   3925     45    184]\n",
            " [   112     43     18  10748     60]\n",
            " [   480     77     64     82   7474]]\n",
            "Validating..\n",
            "Validation loss:  0.2561069109610149\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9375178258566598\n",
            "\t f1 :  [0.68987013 0.97143398 0.74773756 0.67160612 0.81728778]\n",
            "\t average f1 :  0.77958711413525\n",
            "\t confusion matrix :  [[ 1328   592    42    48   240]\n",
            " [   59 41046    37    12    10]\n",
            " [   55   238   661    14    39]\n",
            " [   34  1221     9  1405    21]\n",
            " [  124   245    12    15  1579]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "# FFNN results in higer accuarcy, f score and confusion matrix as NER task don't require to know all words and FFNN with suitable context window size preforms more better and results in higher accuracy and f score\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "# Bidirectional LSTMs preforms slightly better than Unidirectional LSTMs because as we proved that passing all words to the model does not help the model to determine the right tag but also may introduce noise or irrelevant information\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}