{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamedragih1/Named-Entity-Recognition-NER-Models-Implementation/blob/main/A3P2_Named_Entity_Recognition_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c917d04-cd41-410e-81f2-8ae44d04fd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860056ab-ad3a-40a9-ea88-bc2dafd4e81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "3ca5b8ce-16a7-4f4d-88e2-7af448027a97"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-725a471c2cfc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Multiply two matrices on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d28eab5-513b-4579-a0d2-560f6423c11d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749aa5c4-df64-4f05-efbb-ebe89d568869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "\n",
        "        for index, (word_list, tag_list) in enumerate(zip(words, tags)):\n",
        "          word_index_tensor = torch.tensor([self.words_vocab[word] if word in self.words_vocab else self.unknown_idx for word in word_list])\n",
        "          tag_index_tensor = torch.tensor([self.tags_vocab[tag] for tag in tag_list])\n",
        "          word_idxs[index, :len(word_list)] = word_index_tensor\n",
        "          tag_idxs[index, :len(tag_list)] = tag_index_tensor\n",
        "          valid_mask[index, :len(tag_list)] = 1\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2644b2ec-6289-4248-e610-8e5217e258c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        self.embedding = nn.Embedding(len(words_vocab),d_emb)\n",
        "        self.lstm = nn.LSTM(d_emb,d_hidden,bidirectional=bidirectional)\n",
        "        self.fc1 = nn.Linear((d_hidden + bidirectional*d_hidden),len(tags_vocab))\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        embeddings = self.embedding(word_idxs)\n",
        "        lstm_output, _ = self.lstm(embeddings)\n",
        "        logits = self.fc1(lstm_output)\n",
        "\n",
        "\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a199cf3f-55ef-47ad-92df-a11ecca22f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, bidirectional=True)\n",
            "  (fc1): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 17])\n",
            "Input valid_mask shape: torch.Size([4, 17])\n",
            "Output logits shape: torch.Size([4, 17, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        logits = model(word_idxs, valid_mask)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tag_idxs.view(-1), reduction='none')\n",
        "        loss = torch.sum(loss * valid_mask.view(-1)) / torch.sum(valid_mask)\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            logits = model(word_idxs, valid_mask)\n",
        "            loss = nn.CrossEntropyLoss(reduction='none')(logits.view(-1, logits.shape[-1]), tag_idxs.view(-1))\n",
        "            loss = torch.sum(loss * valid_mask.view(-1)) / torch.sum(valid_mask)\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0a0e50-504a-414f-9a40-3d02d05048f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, bidirectional=True)\n",
            "  (fc1): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7476759597107216\n",
            "Training metrics:\n",
            "\t accuracy :  0.7963828236539172\n",
            "\t f1 :  [0.08790957 0.89406816 0.01638028 0.10468171 0.22027108]\n",
            "\t average f1 :  0.26466215841408586\n",
            "\t confusion matrix :  [[   766   8071    109    414    475]\n",
            " [  5425 156647   1433   2076   1251]\n",
            " [   303   3593     51    282    263]\n",
            " [   593   9256     87    777    268]\n",
            " [   505   6015     55    315   1292]]\n",
            "Validating..\n",
            "Validation loss:  0.515448774610247\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8718983009412052\n",
            "\t f1 :  [0.13283302 0.95104407 0.00790514 0.26910657 0.57825203]\n",
            "\t average f1 :  0.3878281650042994\n",
            "\t confusion matrix :  [[  177  1125     0   517   431]\n",
            " [   28 40854     0   242    40]\n",
            " [   53   404     4   327   219]\n",
            " [   56  1876     0   625   133]\n",
            " [  101   491     1   244  1138]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.29735359163196\n",
            "Training metrics:\n",
            "\t accuracy :  0.8992997931773442\n",
            "\t f1 :  [0.37962796 0.9726883  0.3083716  0.51438627 0.64242203]\n",
            "\t average f1 :  0.5634992322942669\n",
            "\t confusion matrix :  [[  3153   2449    136   2643   1482]\n",
            " [   491 165304     27   1108    176]\n",
            " [   828    888    873   1208    742]\n",
            " [  1357   3157     44   5846    558]\n",
            " [   919    987     43    963   5273]]\n",
            "Validating..\n",
            "Validation loss:  0.46196024332727703\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9063480422116286\n",
            "\t f1 :  [0.34722222 0.96508887 0.56793003 0.47609016 0.73191046]\n",
            "\t average f1 :  0.617648349173578\n",
            "\t confusion matrix :  [[  525   767   120   539   299]\n",
            " [   27 40941    21   152    23]\n",
            " [   49   269   487   104    98]\n",
            " [   83  1402    34  1130    41]\n",
            " [   90   301    46   132  1406]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.18879638336322926\n",
            "Training metrics:\n",
            "\t accuracy :  0.9416479912953637\n",
            "\t f1 :  [0.61585334 0.98760254 0.66709167 0.74322183 0.78155707]\n",
            "\t average f1 :  0.7590652904272555\n",
            "\t confusion matrix :  [[  5862   1079    362   1579    954]\n",
            " [   432 165537     75    753     74]\n",
            " [   690    422   2616    459    312]\n",
            " [  1254    986    132   8443    165]\n",
            " [   963    335    159    506   6204]]\n",
            "Validating..\n",
            "Validation loss:  0.40902420026915415\n",
            "Validation metrics:\n",
            "\t accuracy :  0.924397995355091\n",
            "\t f1 :  [0.59328452 0.96887302 0.68295521 0.57815443 0.76990424]\n",
            "\t average f1 :  0.7186342836636888\n",
            "\t confusion matrix :  [[ 1175   643    63   164   205]\n",
            " [   71 40978    24    79    12]\n",
            " [   99   246   587    38    37]\n",
            " [  143  1290    10  1228    19]\n",
            " [  223   268    28    49  1407]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.1280659508925897\n",
            "Training metrics:\n",
            "\t accuracy :  0.9619302520672712\n",
            "\t f1 :  [0.74044742 0.99265859 0.7677668  0.86793961 0.82210967]\n",
            "\t average f1 :  0.838184417302597\n",
            "\t confusion matrix :  [[  7199    714    319    773    880]\n",
            " [   296 165907     77    421     40]\n",
            " [   493    326   3187    277    247]\n",
            " [   619    390     74   9773     79]\n",
            " [   953    190    115    341   6574]]\n",
            "Validating..\n",
            "Validation loss:  0.4029624568564551\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9312227519048201\n",
            "\t f1 :  [0.63817514 0.97021035 0.72421525 0.62804445 0.79340141]\n",
            "\t average f1 :  0.750809321851441\n",
            "\t confusion matrix :  [[ 1217   617    60   108   248]\n",
            " [   68 41004    36    39    17]\n",
            " [   51   239   646    30    41]\n",
            " [   73  1245    21  1328    23]\n",
            " [  155   257    14    34  1515]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.09773127624282131\n",
            "Training metrics:\n",
            "\t accuracy :  0.9710955571508335\n",
            "\t f1 :  [0.79345075 0.99478726 0.80880459 0.92065193 0.84860508]\n",
            "\t average f1 :  0.8732599239328163\n",
            "\t confusion matrix :  [[  7681    581    295    503    812]\n",
            " [   233 166506     92    221     34]\n",
            " [   403    275   3454    176    225]\n",
            " [   351    176     50  10309     74]\n",
            " [   821    133    117    226   6844]]\n",
            "Validating..\n",
            "Validation loss:  0.3853976045336042\n",
            "Validation metrics:\n",
            "\t accuracy :  0.933524833964878\n",
            "\t f1 :  [0.6642655  0.97064251 0.73068182 0.64689397 0.79643147]\n",
            "\t average f1 :  0.7617830533975386\n",
            "\t confusion matrix :  [[ 1291   620    47   103   189]\n",
            " [   60 41031    32    31    10]\n",
            " [   49   244   643    35    36]\n",
            " [   40  1235    14  1385    16]\n",
            " [  197   250    17    38  1473]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.08107893789807956\n",
            "Training metrics:\n",
            "\t accuracy :  0.9761096579677735\n",
            "\t f1 :  [0.82554874 0.99571023 0.83348881 0.94583184 0.86821135]\n",
            "\t average f1 :  0.8937581929658045\n",
            "\t confusion matrix :  [[  7936    522    279    348    766]\n",
            " [   194 166425     92    126     25]\n",
            " [   324    247   3574    135    217]\n",
            " [   209    125     35  10529     56]\n",
            " [   712    103     99    172   7082]]\n",
            "Validating..\n",
            "Validation loss:  0.3815896660089493\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9351138817585463\n",
            "\t f1 :  [0.67509025 0.97096614 0.74485825 0.65022964 0.80448384]\n",
            "\t average f1 :  0.7691256237237243\n",
            "\t confusion matrix :  [[ 1309   599    51    51   240]\n",
            " [   67 41034    38    14    11]\n",
            " [   42   237   670    18    40]\n",
            " [   63  1240    15  1345    27]\n",
            " [  147   248    18    19  1543]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.07186191680806654\n",
            "Training metrics:\n",
            "\t accuracy :  0.9783866982804997\n",
            "\t f1 :  [0.8394921  0.9960782  0.85139639 0.9579566  0.87495412]\n",
            "\t average f1 :  0.9039754804956605\n",
            "\t confusion matrix :  [[  8099    518    257    248    745]\n",
            " [   183 166868     80     86     22]\n",
            " [   289    233   3704     97    208]\n",
            " [   173     94     37  10595     62]\n",
            " [   684     98     92    133   7151]]\n",
            "Validating..\n",
            "Validation loss:  0.38308695810181753\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9359898952858249\n",
            "\t f1 :  [0.68220668 0.97089997 0.75125209 0.6563632  0.81052906]\n",
            "\t average f1 :  0.774250199416781\n",
            "\t confusion matrix :  [[ 1317   597    51    45   240]\n",
            " [   68 41038    34    13    11]\n",
            " [   38   244   675    15    35]\n",
            " [   55  1244    11  1359    21]\n",
            " [  133   249    19    19  1555]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.06544291034892753\n",
            "Training metrics:\n",
            "\t accuracy :  0.9799372007927277\n",
            "\t f1 :  [0.85117222 0.9962198  0.86009411 0.96582628 0.88247423]\n",
            "\t average f1 :  0.9111573271202904\n",
            "\t confusion matrix :  [[  8187    505    238    192    747]\n",
            " [   184 166423     93     57     31]\n",
            " [   250    244   3747     68    204]\n",
            " [   144     71     35  10669     65]\n",
            " [   603     78     87    123   7276]]\n",
            "Validating..\n",
            "Validation loss:  0.3291384364877428\n",
            "Validation metrics:\n",
            "\t accuracy :  0.936315853807603\n",
            "\t f1 :  [0.68715084 0.97126287 0.74874652 0.66793169 0.80150013]\n",
            "\t average f1 :  0.7753184100370965\n",
            "\t confusion matrix :  [[ 1353   586    48    60   203]\n",
            " [   73 41031    35    16     9]\n",
            " [   37   242   672    22    34]\n",
            " [   37  1221     8  1408    16]\n",
            " [  188   246    25    20  1496]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.061018649212740084\n",
            "Training metrics:\n",
            "\t accuracy :  0.9812025016819077\n",
            "\t f1 :  [0.8597966  0.99644245 0.86737613 0.96923844 0.88988764]\n",
            "\t average f1 :  0.9165482506151086\n",
            "\t confusion matrix :  [[  8285    500    240    177    683]\n",
            " [   170 166795     82     47     25]\n",
            " [   224    233   3790     71    201]\n",
            " [   137     53     25  10697     68]\n",
            " [   571     81     83    101   7326]]\n",
            "Validating..\n",
            "Validation loss:  0.36602069650377544\n",
            "Validation metrics:\n",
            "\t accuracy :  0.936478833068492\n",
            "\t f1 :  [0.6787717  0.9710049  0.74680022 0.65816695 0.81955465]\n",
            "\t average f1 :  0.7748596813195381\n",
            "\t confusion matrix :  [[ 1271   607    54    60   258]\n",
            " [   51 41057    36     9    11]\n",
            " [   38   250   671    12    36]\n",
            " [   43  1242    11  1368    26]\n",
            " [   92   246    18    18  1601]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.05828464334761655\n",
            "Training metrics:\n",
            "\t accuracy :  0.9814579185565518\n",
            "\t f1 :  [0.85877243 0.99658711 0.87457396 0.97112766 0.88813723]\n",
            "\t average f1 :  0.9178396788998343\n",
            "\t confusion matrix :  [[  8206    478    256    170    745]\n",
            " [   150 166590     71     47     35]\n",
            " [   207    223   3849     56    187]\n",
            " [   130     53     23  10696     64]\n",
            " [   563     84     81     93   7352]]\n",
            "Validating..\n",
            "Validation loss:  0.3629478918654578\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9362751089923808\n",
            "\t f1 :  [0.68668669 0.97110961 0.75321768 0.66906994 0.79738206]\n",
            "\t average f1 :  0.7754931948843697\n",
            "\t confusion matrix :  [[ 1372   609    44    50   175]\n",
            " [   57 41059    36     3     9]\n",
            " [   42   248   673    11    33]\n",
            " [   45  1230    10  1392    13]\n",
            " [  230   251    17    15  1462]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.05491051133032198\n",
            "Training metrics:\n",
            "\t accuracy :  0.982372026494291\n",
            "\t f1 :  [0.86689954 0.99662417 0.87925767 0.97392095 0.89415749]\n",
            "\t average f1 :  0.9221719612612389\n",
            "\t confusion matrix :  [[  8314    477    248    132    690]\n",
            " [   157 166801     77     38     34]\n",
            " [   187    229   3885     55    179]\n",
            " [   125     45     17  10718     67]\n",
            " [   537     73     75     95   7392]]\n",
            "Validating..\n",
            "Validation loss:  0.3185385359185083\n",
            "Validation metrics:\n",
            "\t accuracy :  0.937090005296826\n",
            "\t f1 :  [0.68060021 0.97112302 0.75453048 0.66456617 0.82029278]\n",
            "\t average f1 :  0.7782225318560875\n",
            "\t confusion matrix :  [[ 1270   601    58    40   281]\n",
            " [   62 41045    39     3    15]\n",
            " [   28   246   687     8    38]\n",
            " [   51  1227    13  1371    28]\n",
            " [   71   248    17    14  1625]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.05365371317775161\n",
            "Training metrics:\n",
            "\t accuracy :  0.9825479581909348\n",
            "\t f1 :  [0.86741573 0.9966811  0.88225314 0.97407289 0.89497497]\n",
            "\t average f1 :  0.9230795642812634\n",
            "\t confusion matrix :  [[  8299    485    238    131    727]\n",
            " [   152 166669     66     42     31]\n",
            " [   176    220   3900     44    188]\n",
            " [   128     48     27  10651     64]\n",
            " [   500     66     82     83   7418]]\n",
            "Validating..\n",
            "Validation loss:  0.30670290333884104\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9372122397424928\n",
            "\t f1 :  [0.69389839 0.97126355 0.74972191 0.66842105 0.81479524]\n",
            "\t average f1 :  0.7796200306918668\n",
            "\t confusion matrix :  [[ 1359   586    45    50   210]\n",
            " [   72 41032    39    11    10]\n",
            " [   45   245   674    11    32]\n",
            " [   44  1222    11  1397    16]\n",
            " [  147   243    22    21  1542]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.05189309338176692\n",
            "Training metrics:\n",
            "\t accuracy :  0.9831716857251193\n",
            "\t f1 :  [0.87277101 0.99670994 0.88321995 0.97513208 0.90158119]\n",
            "\t average f1 :  0.9258828351466363\n",
            "\t confusion matrix :  [[  8345    474    238    129    678]\n",
            " [   160 166620     65     35     39]\n",
            " [   176    222   3895     48    186]\n",
            " [   125     39     25  10705     64]\n",
            " [   453     66     70     81   7498]]\n",
            "Validating..\n",
            "Validation loss:  0.3076118252107075\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9371307501120483\n",
            "\t f1 :  [0.69467787 0.97119332 0.74630137 0.67129519 0.81098546]\n",
            "\t average f1 :  0.7788906415269797\n",
            "\t confusion matrix :  [[ 1364   607    48    49   182]\n",
            " [   59 41047    44     7     7]\n",
            " [   45   241   681     9    31]\n",
            " [   41  1222    12  1402    13]\n",
            " [  168   248    33    20  1506]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0510731658173932\n",
            "Training metrics:\n",
            "\t accuracy :  0.9833119576011937\n",
            "\t f1 :  [0.87591773 0.99667319 0.88793103 0.97521866 0.89957071]\n",
            "\t average f1 :  0.9270622646873903\n",
            "\t confusion matrix :  [[  8411    475    214    122    666]\n",
            " [   137 166571     71     43     39]\n",
            " [   176    222   3914     39    174]\n",
            " [   111     49     24  10704     69]\n",
            " [   482     76     68     87   7439]]\n",
            "Validating..\n",
            "Validation loss:  0.28811303632599966\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9376400603023265\n",
            "\t f1 :  [0.69414204 0.97126831 0.75582025 0.67494626 0.81399046]\n",
            "\t average f1 :  0.7820334654320653\n",
            "\t confusion matrix :  [[ 1339   599    59    46   207]\n",
            " [   60 41039    46    10     9]\n",
            " [   29   235   698    10    35]\n",
            " [   31  1222    12  1413    12]\n",
            " [  149   247    25    18  1536]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.0497614643364041\n",
            "Training metrics:\n",
            "\t accuracy :  0.9834541773464849\n",
            "\t f1 :  [0.87484316 0.99674731 0.88597582 0.97675897 0.90144245]\n",
            "\t average f1 :  0.9271535405034795\n",
            "\t confusion matrix :  [[  8367    486    242    120    667]\n",
            " [   140 166702     71     34     37]\n",
            " [   168    210   3920     42    174]\n",
            " [   112     32     28  10759     61]\n",
            " [   459     78     74     83   7468]]\n",
            "Validating..\n",
            "Validation loss:  0.30417661581720623\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9368251639978813\n",
            "\t f1 :  [0.69818716 0.97136908 0.75528365 0.67243036 0.79300777]\n",
            "\t average f1 :  0.7780556029662817\n",
            "\t confusion matrix :  [[ 1425   595    42    39   149]\n",
            " [   64 41052    35     6     7]\n",
            " [   47   242   679    10    29]\n",
            " [   41  1225     9  1400    15]\n",
            " [  255   246    26    19  1429]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9678aa-4232-49e5-ac30-7de6a8316549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128)\n",
            "  (fc1): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.65347178445922\n",
            "Training metrics:\n",
            "\t accuracy :  0.7810388250255592\n",
            "\t f1 :  [0.12505996 0.88648465 0.04761399 0.04038347 0.17845152]\n",
            "\t average f1 :  0.25559871738422585\n",
            "\t confusion matrix :  [[  1434   7840    230    115    247]\n",
            " [  8670 153837   3968    268    201]\n",
            " [   643   3426    224     46    178]\n",
            " [  1253   9107    264    238    154]\n",
            " [  1067   5918    206    104    877]]\n",
            "Validating..\n",
            "Validation loss:  0.37704165492738995\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8741800105936519\n",
            "\t f1 :  [0.30837526 0.9479257  0.03508772 0.18974076 0.56418733]\n",
            "\t average f1 :  0.40906335291075824\n",
            "\t confusion matrix :  [[  521  1232     1   196   300]\n",
            " [   59 41003     0    67    35]\n",
            " [  165   484    18   154   186]\n",
            " [  180  2056     0   344   110]\n",
            " [  204   572     0   175  1024]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.29213951859209275\n",
            "Training metrics:\n",
            "\t accuracy :  0.901020387827092\n",
            "\t f1 :  [0.4283372  0.97358244 0.34154807 0.50996124 0.63212881]\n",
            "\t average f1 :  0.5771115511214007\n",
            "\t confusion matrix :  [[  3788   2199    127   2332   1407]\n",
            " [   509 165270     18   1212    182]\n",
            " [   895    842    984   1009    797]\n",
            " [  1600   3101     36   5657    581]\n",
            " [  1042    906     70   1001   5143]]\n",
            "Validating..\n",
            "Validation loss:  0.27861158549785614\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9016623884610683\n",
            "\t f1 :  [0.52671951 0.96675424 0.55213115 0.5551969  0.72660447]\n",
            "\t average f1 :  0.6654812533129789\n",
            "\t confusion matrix :  [[ 1030   315    48   667   190]\n",
            " [   77 39373    16  1688    10]\n",
            " [  182   121   421   243    40]\n",
            " [  135   365     3  2150    37]\n",
            " [  237   116    30   307  1285]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.18126325088518638\n",
            "Training metrics:\n",
            "\t accuracy :  0.9455177261827011\n",
            "\t f1 :  [0.66320591 0.98819942 0.67737789 0.76996628 0.77242958]\n",
            "\t average f1 :  0.7742358168139678\n",
            "\t confusion matrix :  [[  6324    897    307   1376    964]\n",
            " [   351 165599     84    971     65]\n",
            " [   642    412   2635    512    316]\n",
            " [   827    908     62   9019    153]\n",
            " [  1059    267    175    580   6074]]\n",
            "Validating..\n",
            "Validation loss:  0.22993504575320653\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9241942712789798\n",
            "\t f1 :  [0.62054934 0.97252961 0.67471264 0.64975732 0.7611577 ]\n",
            "\t average f1 :  0.7357413206645804\n",
            "\t confusion matrix :  [[ 1220   404    58   349   219]\n",
            " [   72 40076    29   975    12]\n",
            " [   83   166   587   124    47]\n",
            " [   85   479    14  2075    37]\n",
            " [  222   127    45   174  1407]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.12411160905052114\n",
            "Training metrics:\n",
            "\t accuracy :  0.9644229329873759\n",
            "\t f1 :  [0.76597735 0.99286402 0.77686944 0.87971605 0.82772351]\n",
            "\t average f1 :  0.8486300739649943\n",
            "\t confusion matrix :  [[  7371    664    283    698    861]\n",
            " [   250 165988    101    482     38]\n",
            " [   436    306   3231    288    254]\n",
            " [   410    394     49  10038     87]\n",
            " [   902    151    139    337   6652]]\n",
            "Validating..\n",
            "Validation loss:  0.22791043988295964\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9201197897567535\n",
            "\t f1 :  [0.65519013 0.9707012  0.71299435 0.62953232 0.78396072]\n",
            "\t average f1 :  0.7504757445822708\n",
            "\t confusion matrix :  [[ 1275   247    56   479   193]\n",
            " [   64 39426    25  1641     8]\n",
            " [   45   105   631   189    37]\n",
            " [   45   222    11  2396    16]\n",
            " [  213    68    40   217  1437]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.09709764658300965\n",
            "Training metrics:\n",
            "\t accuracy :  0.9723482318222151\n",
            "\t f1 :  [0.80719875 0.99481206 0.81211693 0.92615001 0.8519407 ]\n",
            "\t average f1 :  0.8784436900453405\n",
            "\t confusion matrix :  [[  7737    595    260    454    816]\n",
            " [   216 166539     87    234     34]\n",
            " [   336    294   3445    202    243]\n",
            " [   241    174     47  10409     78]\n",
            " [   778    103    125    230   6925]]\n",
            "Validating..\n",
            "Validation loss:  0.22424350466047013\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9177973352890845\n",
            "\t f1 :  [0.66239658 0.96993854 0.72534819 0.62162489 0.80031283]\n",
            "\t average f1 :  0.7559242058608656\n",
            "\t confusion matrix :  [[ 1241   183    52   534   240]\n",
            " [   53 39057    37  2008     9]\n",
            " [   37    66   651   205    48]\n",
            " [   36    48    10  2567    29]\n",
            " [  130    17    38   255  1535]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.08215194774998559\n",
            "Training metrics:\n",
            "\t accuracy :  0.9757728360590348\n",
            "\t f1 :  [0.82358491 0.99557213 0.82855471 0.94813222 0.86357841]\n",
            "\t average f1 :  0.8918844753574069\n",
            "\t confusion matrix :  [[  7857    580    252    322    818]\n",
            " [   199 166608    103    111     33]\n",
            " [   314    264   3540    147    240]\n",
            " [   179    107     34  10584     78]\n",
            " [   702     85    111    180   7112]]\n",
            "Validating..\n",
            "Validation loss:  0.22098359252725328\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9159027013812492\n",
            "\t f1 :  [0.67048114 0.96655815 0.74282561 0.59931955 0.80169447]\n",
            "\t average f1 :  0.7561757842141965\n",
            "\t confusion matrix :  [[ 1289   262    53   432   214]\n",
            " [   56 39192    46  1863     7]\n",
            " [   36    82   673   173    43]\n",
            " [   39   327    10  2290    24]\n",
            " [  175    69    23   194  1514]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.07301983264861284\n",
            "Training metrics:\n",
            "\t accuracy :  0.9782955713188272\n",
            "\t f1 :  [0.837739   0.99596674 0.85004066 0.95973728 0.87496208]\n",
            "\t average f1 :  0.903689152390891\n",
            "\t confusion matrix :  [[  8018    572    249    249    764]\n",
            " [   179 166560     87     74     27]\n",
            " [   268    258   3659     93    225]\n",
            " [   166     71     24  10667     74]\n",
            " [   659     81     87    144   7211]]\n",
            "Validating..\n",
            "Validation loss:  0.22611080535820552\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9178380801043067\n",
            "\t f1 :  [0.67423442 0.96523856 0.74546454 0.57830591 0.80604009]\n",
            "\t average f1 :  0.7538567058239766\n",
            "\t confusion matrix :  [[ 1277   378    52   301   242]\n",
            " [   49 39652    47  1407     9]\n",
            " [   31   133   678   127    38]\n",
            " [   41   712    10  1898    29]\n",
            " [  140   121    25   141  1548]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.06666377780062181\n",
            "Training metrics:\n",
            "\t accuracy :  0.9798849858102615\n",
            "\t f1 :  [0.85030316 0.99623422 0.85629086 0.96604868 0.88235294]\n",
            "\t average f1 :  0.9102459720035594\n",
            "\t confusion matrix :  [[  8134    535    254    191    746]\n",
            " [   150 166666     96     51     27]\n",
            " [   249    250   3733     83    215]\n",
            " [   143     65     26  10656     71]\n",
            " [   596     86     80    119   7275]]\n",
            "Validating..\n",
            "Validation loss:  0.22559625123228347\n",
            "Validation metrics:\n",
            "\t accuracy :  0.920669844762254\n",
            "\t f1 :  [0.68150772 0.96703753 0.74818537 0.61172001 0.8009964 ]\n",
            "\t average f1 :  0.7618894039391928\n",
            "\t confusion matrix :  [[ 1347   383    50   329   141]\n",
            " [   53 39635    29  1443     4]\n",
            " [   38   128   670   139    32]\n",
            " [   30   545     8  2093    14]\n",
            " [  235   117    27   149  1447]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.06281235648526086\n",
            "Training metrics:\n",
            "\t accuracy :  0.9807171314741036\n",
            "\t f1 :  [0.8560693  0.99627199 0.863626   0.96870046 0.88828502]\n",
            "\t average f1 :  0.9145905539931307\n",
            "\t confusion matrix :  [[  8202    534    255    183    712]\n",
            " [   153 166891     81     58     31]\n",
            " [   222    257   3787     68    195]\n",
            " [   120     57     34  10693     69]\n",
            " [   579     78     84    102   7355]]\n",
            "Validating..\n",
            "Validation loss:  0.22894297753061568\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9265370981542599\n",
            "\t f1 :  [0.67855242 0.96847604 0.75250836 0.62972208 0.81390593]\n",
            "\t average f1 :  0.7686329672660105\n",
            "\t confusion matrix :  [[ 1275   442    50   216   267]\n",
            " [   54 40046    40  1014    10]\n",
            " [   36   160   675    93    43]\n",
            " [   34   731     8  1892    25]\n",
            " [  109   156    14   104  1592]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.05932489658395449\n",
            "Training metrics:\n",
            "\t accuracy :  0.9815091082360807\n",
            "\t f1 :  [0.86154328 0.99639546 0.87044256 0.97084297 0.89275397]\n",
            "\t average f1 :  0.9183956490153399\n",
            "\t confusion matrix :  [[  8251    517    234    157    723]\n",
            " [   163 166548     78     50     30]\n",
            " [   215    242   3796     71    187]\n",
            " [   121     50     26  10705     70]\n",
            " [   522     75     77     98   7417]]\n",
            "Validating..\n",
            "Validation loss:  0.24315517927919114\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9248869331377583\n",
            "\t f1 :  [0.69067688 0.96733398 0.76097561 0.60924441 0.80326087]\n",
            "\t average f1 :  0.7662983524376661\n",
            "\t confusion matrix :  [[ 1352   450    52   216   180]\n",
            " [   56 40081    44   976     7]\n",
            " [   22   155   702    96    32]\n",
            " [   38   847    11  1786     8]\n",
            " [  197   172    29    99  1478]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.056434844379071834\n",
            "Training metrics:\n",
            "\t accuracy :  0.9823504154018213\n",
            "\t f1 :  [0.86769939 0.9965429  0.87640705 0.97329026 0.89681344]\n",
            "\t average f1 :  0.9221506089658483\n",
            "\t confusion matrix :  [[  8290    505    249    134    688]\n",
            " [   140 166470     69     49     33]\n",
            " [   192    235   3854     55    181]\n",
            " [   119     48     27  10695     67]\n",
            " [   501     76     79     88   7444]]\n",
            "Validating..\n",
            "Validation loss:  0.2513625792094639\n",
            "Validation metrics:\n",
            "\t accuracy :  0.918347390294585\n",
            "\t f1 :  [0.68277144 0.96597922 0.75384615 0.59974871 0.8126757 ]\n",
            "\t average f1 :  0.7630042424118966\n",
            "\t confusion matrix :  [[ 1286   299    48   350   267]\n",
            " [   63 39368    53  1663    17]\n",
            " [   33    98   686   146    44]\n",
            " [   29   483    10  2148    20]\n",
            " [  106    97    16   166  1590]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.05511542561429518\n",
            "Training metrics:\n",
            "\t accuracy :  0.9822542229856246\n",
            "\t f1 :  [0.86585111 0.99656905 0.87931624 0.9732402  0.89461302]\n",
            "\t average f1 :  0.9219179232092575\n",
            "\t confusion matrix :  [[  8281    497    232    148    725]\n",
            " [   158 166436     72     44     33]\n",
            " [   192    223   3858     54    183]\n",
            " [   113     46     28  10729     71]\n",
            " [   501     73     75     86   7415]]\n",
            "Validating..\n",
            "Validation loss:  0.2782382539340428\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9370288880739925\n",
            "\t f1 :  [0.6832     0.97106505 0.75013669 0.6645768  0.81970596]\n",
            "\t average f1 :  0.7777368989826057\n",
            "\t confusion matrix :  [[ 1281   616    70    40   243]\n",
            " [   48 41061    34    10    11]\n",
            " [   31   243   686    11    36]\n",
            " [   37  1241    11  1378    23]\n",
            " [  103   244    21    18  1589]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.05340378800476039\n",
            "Training metrics:\n",
            "\t accuracy :  0.9827426071845882\n",
            "\t f1 :  [0.87108598 0.99653012 0.88231293 0.97456432 0.89802552]\n",
            "\t average f1 :  0.9245037720896951\n",
            "\t confusion matrix :  [[  8318    504    236    127    686]\n",
            " [   157 166429     66     48     38]\n",
            " [   170    222   3891     48    190]\n",
            " [   103     51     30  10709     73]\n",
            " [   479     73     76     79   7459]]\n",
            "Validating..\n",
            "Validation loss:  0.2662679467882429\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9370288880739925\n",
            "\t f1 :  [0.68961947 0.971238   0.75371901 0.66969624 0.80755922]\n",
            "\t average f1 :  0.7783663877645389\n",
            "\t confusion matrix :  [[ 1332   612    48    50   208]\n",
            " [   53 41062    34     6     9]\n",
            " [   31   245   684    14    33]\n",
            " [   36  1228    11  1400    15]\n",
            " [  161   245    31    21  1517]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.05198742587257315\n",
            "Training metrics:\n",
            "\t accuracy :  0.983188625516784\n",
            "\t f1 :  [0.87313472 0.99670506 0.88436758 0.97621538 0.89964567]\n",
            "\t average f1 :  0.9260136812993668\n",
            "\t confusion matrix :  [[  8338    476    236    124    704]\n",
            " [   143 166675     68     43     39]\n",
            " [   181    213   3912     39    177]\n",
            " [   105     43     29  10733     58]\n",
            " [   454     77     80     82   7490]]\n",
            "Validating..\n",
            "Validation loss:  0.25532183051109314\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9373141017805484\n",
            "\t f1 :  [0.692971   0.97118981 0.75027747 0.66666667 0.81895442]\n",
            "\t average f1 :  0.7800118718959039\n",
            "\t confusion matrix :  [[ 1326   589    50    46   239]\n",
            " [   65 41025    43    21    10]\n",
            " [   34   242   676    13    42]\n",
            " [   43  1219     9  1392    27]\n",
            " [  109   245    17    14  1590]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.050567134800884456\n",
            "Training metrics:\n",
            "\t accuracy :  0.9835221318429064\n",
            "\t f1 :  [0.87557797 0.99670911 0.88767869 0.97580314 0.90348413]\n",
            "\t average f1 :  0.927850610697418\n",
            "\t confusion matrix :  [[  8332    474    230    133    689]\n",
            " [   145 166578     70     40     41]\n",
            " [   156    217   3912     49    172]\n",
            " [   109     40     24  10707     73]\n",
            " [   432     73     72     63   7559]]\n",
            "Validating..\n",
            "Validation loss:  0.26473882368632723\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9372937293729373\n",
            "\t f1 :  [0.69737519 0.97117433 0.75300546 0.66730447 0.81391027]\n",
            "\t average f1 :  0.780553946042738\n",
            "\t confusion matrix :  [[ 1355   599    55    44   197]\n",
            " [   59 41036    42    18     9]\n",
            " [   30   239   689    15    34]\n",
            " [   41  1224    11  1395    19]\n",
            " [  151   246    26    19  1533]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": [
        "# FFNN results in higer accuarcy, f score and confusion matrix as NER task don't require to know all words and FFNN with suitable context window size preforms more better and results in higher accuracy and f score\n",
        "# Unidirectional LSTMs preforms slightly better than Bidirectional LSTMs because as we proved that passing all words to the model does not help the model to determine the right tag but also may introduce noise or irrelevant information"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}